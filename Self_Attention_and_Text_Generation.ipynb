{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86373b4d",
   "metadata": {},
   "source": [
    "# Self-Attention Mechanisms and Text Generation\n",
    "\n",
    "This project implements and demonstrates two fundamental components of modern NLP: self-attention mechanisms and transformer-based text generation.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project showcases advanced **natural language processing** techniques, focusing on the core building blocks of modern transformer architectures and their applications.\n",
    "\n",
    "## Part I: Self-Attention Mechanism Implementation\n",
    "\n",
    "### **Single-Head Self-Attention**\n",
    "- **Architecture**: Query-Key-Value attention mechanism\n",
    "- **Key Components**:\n",
    "  - Linear projections for Q, K, V transformations\n",
    "  - Scaled dot-product attention computation\n",
    "  - Softmax normalization with dropout regularization\n",
    "  - Output projection layer\n",
    "- **Features**: Debug mode for tensor shape visualization\n",
    "- **Applications**: Foundation for transformer-based models\n",
    "\n",
    "### **Multi-Head Self-Attention**\n",
    "- **Architecture**: Parallel attention heads with different learned representations\n",
    "- **Key Improvements**:\n",
    "  - Multiple attention heads capture different types of relationships\n",
    "  - Head-wise dimension splitting and recombination\n",
    "  - Enhanced representational capacity\n",
    "  - Efficient parallel computation\n",
    "- **Implementation**: Proper tensor reshaping and head management\n",
    "- **Scalability**: Configurable number of attention heads\n",
    "\n",
    "### **Technical Features**:\n",
    "- **Mathematical Foundation**: Scaled dot-product attention with √d_k scaling\n",
    "- **Memory Efficiency**: Optimized tensor operations and reshaping\n",
    "- **Debugging Support**: Shape tracking and intermediate tensor visualization\n",
    "- **Modularity**: Clean, reusable attention module design\n",
    "- **PyTorch Integration**: Native tensor operations and automatic differentiation\n",
    "\n",
    "## Part II: Transformer-Based Text Generation\n",
    "\n",
    "### **Model**: Large Language Model with 4-bit Quantization\n",
    "- **Base Model**: Pre-trained transformer language model\n",
    "- **Optimization**: 4-bit quantization for memory efficiency\n",
    "- **Technology**: BitsAndBytes integration for reduced memory footprint\n",
    "- **Acceleration**: GPU acceleration support\n",
    "\n",
    "### **Text Generation Pipeline**:\n",
    "- **Tokenization**: Advanced tokenizer for text preprocessing\n",
    "- **Prompt Engineering**: Structured prompt creation and formatting\n",
    "- **Generation Strategy**: Configurable sampling parameters\n",
    "- **Post-processing**: Clean text extraction and formatting\n",
    "\n",
    "### **Key Capabilities**:\n",
    "- **Interactive Generation**: Custom prompt-based text generation\n",
    "- **Memory Optimization**: Efficient large model handling\n",
    "- **Flexible Configuration**: Adjustable generation parameters\n",
    "- **Quality Control**: Temperature and sampling controls\n",
    "\n",
    "### **Technical Implementation**:\n",
    "- **Framework**: PyTorch + Transformers library\n",
    "- **Quantization**: 4-bit precision for large model deployment\n",
    "- **Memory Management**: Optimized loading and inference\n",
    "- **GPU Utilization**: Accelerated computation when available\n",
    "\n",
    "### **Applications**:\n",
    "- **Creative Writing**: Story and content generation\n",
    "- **Conversational AI**: Interactive dialogue systems\n",
    "- **Code Generation**: Programming assistance capabilities\n",
    "- **Educational Tools**: Learning and demonstration purposes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ee5b6b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf7a479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-head Self-Attention Implementation\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads=1, dropout=0.0, debug=False):\n",
    "        super().__init__()\n",
    "        assert emb_dim % n_heads == 0\n",
    "        self.emb_dim, self.n_heads = emb_dim, n_heads\n",
    "        self.head_dim  = emb_dim // n_heads\n",
    "        self.scale     = math.sqrt(self.head_dim)\n",
    "        self.debug     = debug\n",
    "\n",
    "        self.q_proj = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _p(self, name, t):\n",
    "        if self.debug:  # print shapes\n",
    "            print(f\"{name:<12}{tuple(t.shape)}\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ---------- Linear projections ----------\n",
    "        Q = self.q_proj(x); self._p(\"Q\", Q)\n",
    "        K = self.k_proj(x); self._p(\"K\", K)\n",
    "        V = self.v_proj(x); self._p(\"V\", V)\n",
    "\n",
    "        # ---------- Scaled dot‑product ----------\n",
    "        scores = Q @ K.transpose(-2, -1) / self.scale  # (B,T,T)\n",
    "        self._p(\"scores\", scores)\n",
    "\n",
    "        # ---------- softmax + dropout -----------\n",
    "        weights = self.dropout(torch.softmax(scores, -1))\n",
    "        self._p(\"weights\", weights)\n",
    "\n",
    "        # ---------- Weighted sum ----------------\n",
    "        context = weights @ V                       # (B,T,D)\n",
    "        self._p(\"context\", context)\n",
    "\n",
    "        # ---------- Final linear ----------------\n",
    "        out = self.o_proj(context)\n",
    "        self._p(\"out\", out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb73073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q           (2, 5, 16)\n",
      "K           (2, 5, 16)\n",
      "V           (2, 5, 16)\n",
      "scores      (2, 5, 5)\n",
      "weights     (2, 5, 5)\n",
      "context     (2, 5, 16)\n",
      "out         (2, 5, 16)\n",
      "return torch.Size([2, 5, 16])\n"
     ]
    }
   ],
   "source": [
    "# Test single-head attention with tensor shape debugging\n",
    "B, T, D = 2, 5, 16\n",
    "x = torch.randn(B, T, D)\n",
    "attn_single = Attention(D, n_heads=1, debug=True)\n",
    "y = attn_single(x)\n",
    "print(\"return\", y.shape)   # (2,5,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140c7150",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head Self-Attention Implementation\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, emb_dim, n_heads=8, dropout=0.0, debug=False):\n",
    "        super().__init__()\n",
    "        assert emb_dim % n_heads == 0\n",
    "        self.emb_dim, self.n_heads = emb_dim, n_heads\n",
    "        self.head_dim = emb_dim // n_heads\n",
    "        self.scale    = math.sqrt(self.head_dim)\n",
    "        self.debug    = debug\n",
    "\n",
    "        self.q_proj = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(emb_dim, emb_dim, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _p(self, name, t):\n",
    "        if self.debug:\n",
    "            print(f\"{name:<12}{tuple(t.shape)}\")\n",
    "\n",
    "    def _split_heads(self, t):\n",
    "        # (B,T,D) -> (B,H,T,d)\n",
    "        B, T, _ = t.shape\n",
    "        return t.view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.shape\n",
    "        # ---------- Linear projections ----------\n",
    "        Q = self._split_heads(self.q_proj(x)); self._p(\"Q\", Q)\n",
    "        K = self._split_heads(self.k_proj(x)); self._p(\"K\", K)\n",
    "        V = self._split_heads(self.v_proj(x)); self._p(\"V\", V)\n",
    "\n",
    "        # ---------- Scaled dot‑product ----------\n",
    "        scores = Q @ K.transpose(-2, -1) / self.scale      # (B,H,T,T)\n",
    "        self._p(\"scores\", scores)\n",
    "\n",
    "        # ---------- softmax + dropout -----------\n",
    "        weights = self.dropout(torch.softmax(scores, -1))  # (B,H,T,T)\n",
    "        self._p(\"weights\", weights)\n",
    "\n",
    "        # ---------- Weighted sum ----------------\n",
    "        ctx = weights @ V                                  # (B,H,T,d)\n",
    "        self._p(\"context\", ctx)\n",
    "\n",
    "        # ---------- Concatenate heads -----------\n",
    "        ctx = ctx.transpose(1, 2).contiguous().view(B, T, self.emb_dim)\n",
    "        self._p(\"concat\", ctx)\n",
    "\n",
    "        out = self.o_proj(ctx)                             # (B,T,D)\n",
    "        self._p(\"out\", out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3763df87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q           (2, 8, 10, 8)\n",
      "K           (2, 8, 10, 8)\n",
      "V           (2, 8, 10, 8)\n",
      "scores      (2, 8, 10, 10)\n",
      "weights     (2, 8, 10, 10)\n",
      "context     (2, 8, 10, 8)\n",
      "concat      (2, 10, 64)\n",
      "out         (2, 10, 64)\n",
      "return torch.Size([2, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "# Test multi-head attention with tensor shape debugging\n",
    "B, T, D, H = 2, 10, 64, 8\n",
    "x = torch.randn(B, T, D)\n",
    "attn_multi = Attention(D, n_heads=H, debug=True)\n",
    "y = attn_multi(x)\n",
    "print(\"return\", y.shape)   # (2,10,64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81399be0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part II: Transformer-Based Text Generation\n",
    "\n",
    "This section demonstrates large language model inference with memory optimization techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86bd220",
   "metadata": {},
   "source": [
    "### Environment Setup for Text Generation\n",
    "\n",
    "This section requires additional packages for transformer models and quantization support. Note that this may require different package versions than the LSTM sentiment analysis project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895ce63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment configuration for text generation (run if needed)\n",
    "# !pip uninstall -y torch torchvision torchaudio\n",
    "# !pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce4a4402",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1\n",
      "[notice] To update, run: C:\\Users\\INK\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -qU transformers bitsandbytes accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96086d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 608, in run_forever\n",
      "    self._run_once()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\base_events.py\", line 1936, in _run_once\n",
      "    handle._run()\n",
      "  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\Lib\\asyncio\\events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3077, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3132, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3336, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3519, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\IPython\\core\\interactiveshell.py\", line 3579, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Temp\\ipykernel_37760\\193614729.py\", line 4, in <module>\n",
      "    from transformers import (\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\__init__.py\", line 26, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\__init__.py\", line 25, in <module>\n",
      "    from .chat_template_utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\utils\\chat_template_utils.py\", line 40, in <module>\n",
      "    from torch import Tensor\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    }
   ],
   "source": [
    "# Import required packages for text generation\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    ")\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90763bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\INK\\.cache\\huggingface\\hub\\models--Qwen--Qwen2.5-3B-Instruct. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Fetching 2 files: 100%|██████████| 2/2 [07:58<00:00, 239.32s/it]\n",
      "Sliding Window Attention is enabled but not implemented for `sdpa`; unexpected results may be encountered.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.70s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2Attention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure 4-bit quantization for memory efficiency\n",
    "bnb_cfg = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,   # keeps maths stable\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_cfg,\n",
    "    device_map=\"auto\",              # spreads across available GPUs\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.eval()                        # inference‑only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5c515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompts for text generation\n",
    "prompts = [\n",
    "        \"What is the capital of France?\",\n",
    "        \"What is the meaning of life?\",\n",
    "        \"What is the best programming language?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5345c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt 1:\n",
      "token ids : [3838, 374, 279, 6722, 315, 9625, 30] ...\n",
      "back‑to‑text: What is the capital of France?\n",
      "\n",
      "Prompt 2:\n",
      "token ids : [3838, 374, 279, 7290, 315, 2272, 30] ...\n",
      "back‑to‑text: What is the meaning of life?\n",
      "\n",
      "Prompt 3:\n",
      "token ids : [3838, 374, 279, 1850, 15473, 4128, 30] ...\n",
      "back‑to‑text: What is the best programming language?\n"
     ]
    }
   ],
   "source": [
    "# Tokenize prompts for model input\n",
    "for i, text in enumerate(prompts, 1):\n",
    "    ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "    print(f\"\\nPrompt {i}:\")\n",
    "    print(\"token ids :\", ids.tolist()[0][:15], \"...\")  # preview first 15 tokens\n",
    "    print(\"back‑to‑text:\", tokenizer.decode(ids[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940f6f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\INK\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\transformers\\integrations\\sdpa_attention.py:54: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Q1 =============================================\n",
      "What is the capital of France?\n",
      "--- A -----------------------------------------------\n",
      "What is the capital of France? The capital of France is Paris.\n",
      "\n",
      "Paris is a beautiful city located in northern France, and it has been the capital of France since 1795. It is known for its art, culture, cuisine, fashion, and historical landmarks such as the Eiffel Tower, Louvre Museum, and Notre-Dame Cathedral. \n",
      "\n",
      "The city has a rich history dating back to Roman times, and it has played a significant role in French politics, art, science, and technology throughout the centuries. Today, Paris continues to be one of the most popular tourist destinations in the world, attracting millions of visitors each year. \n",
      "\n",
      "In addition to\n",
      "\n",
      "=== Q2 =============================================\n",
      "What is the meaning of life?\n",
      "--- A -----------------------------------------------\n",
      "What is the meaning of life? This question has puzzled many people throughout history. It is a philosophical inquiry that deals with the ultimate purpose and meaning of existence. The answer to this question can vary greatly depending on one's personal beliefs, cultural background, and perspective.\n",
      "To answer this question, it is essential to consider different perspectives and ideas. Some believe that the meaning of life lies in finding happiness, while others believe that it is about achieving success or fulfilling one's potential. Some people believe that the meaning of life is to serve a higher power, while others believe that it is about self-improvement and personal growth.\n",
      "The meaning of life can also be seen as a\n",
      "\n",
      "=== Q3 =============================================\n",
      "What is the best programming language?\n",
      "--- A -----------------------------------------------\n",
      "What is the best programming language? What are the pros and cons of each?\n",
      "The question of which programming language is \"best\" is highly subjective and can vary based on the specific needs, preferences, and goals of a particular project or developer. However, there are several popular languages that are often compared for their features, ease of use, performance, and popularity.\n",
      "\n",
      "### 1. **Python**\n",
      "- **Pros:**\n",
      "  - **Ease of Use:** Highly readable and easy to learn.\n",
      "  - **Versatile:** Used in web development, scientific computing, machine learning, data analysis, automation scripts, etc.\n",
      "  - **Community Support:** Large and active community with\n"
     ]
    }
   ],
   "source": [
    "# Generate text using the language model\n",
    "device = model.device               # already on GPU via device_map\n",
    "gen_kwargs = dict(\n",
    "    max_new_tokens=128,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "for i, text in enumerate(prompts, 1):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    gen_ids = model.generate(**inputs, **gen_kwargs)\n",
    "    reply = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "    print(f\"\\n=== Q{i} =============================================\")\n",
    "    print(text)\n",
    "    print(\"--- A -----------------------------------------------\")\n",
    "    print(reply.strip())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
